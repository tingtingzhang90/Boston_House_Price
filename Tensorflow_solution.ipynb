{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last section is dedicated to a TensorFlow solution. This method is sligthly more complicated than the other one.\n",
    "\n",
    "Note that if you use Jupyter notebook, you need to Restart and clean the kernel to run this session.\n",
    "\n",
    "TensorFlow has built a great tool to pass the data into the pipeline. In this section, you will build the input_fn function by yourself.\n",
    "\n",
    "## Step 1) Define the path and the format of the data\n",
    "\n",
    "First of all, you declare two variables with the path of the csv file. Note that, you have two files, one for the training set and one for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "df_train = \"D:/boston/boston_train.csv\"\n",
    "df_eval = \"D:/boston/boston_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to define the columns you want to use from the csv file. We will use all. After that, you need to declare the type of variable it is.\n",
    "\n",
    "Floats variable are defined by [0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\", \"age\",\n",
    "                \"dis\", \"tax\", \"ptratio\", \"medv\"]\n",
    "RECORDS_ALL = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Define the input_fn function\n",
    "\n",
    "The function can be broken into three part:\n",
    "\n",
    "1. Import the data\n",
    "2. Create the iterator\n",
    "3. Consume the data\n",
    "\n",
    "Below is the overal code to define the function. The code will be explained after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_file, batch_size, num_epoch = None):\n",
    "    # Step 1\n",
    "    def parse_csv(value):        \n",
    "        columns = tf.decode_csv(value, record_defaults = RECORDS_ALL)        \n",
    "        features = dict(zip(COLUMNS, columns))\n",
    "        #labels = features.pop('median_house_value')        \n",
    "        labels =  features.pop('medv')        \n",
    "        return features, labels\n",
    "          \n",
    "    # Extract lines from input files using the Dataset API.    \n",
    "    dataset = (tf.data.TextLineDataset(data_file).skip(1).map(parse_csv))\n",
    "          \n",
    "    dataset = dataset.repeat(num_epoch)    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # Step 3    \n",
    "    iterator = dataset.make_one_shot_iterator()    \n",
    "    features, labels = iterator.get_next()    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import the data **\n",
    "\n",
    "For a csv file, the dataset method reads one line at a time. To build the dataset, you need to use the object TextLineDataset. Your dataset has a header so you need to use skip(1) to skip the first line. At this point, you only read the data and exclude the header in the pipeline. To feed the model, you need to separate the features from the label. The method used to apply any transformation to the data is map.\n",
    "\n",
    "This method calls a function that you will create in order to instruct how to transform the data. In a nutshell, you need to pass the data in the TextLineDataset object, exclude the header and apply a transformation which is instructed by a function.\n",
    "\n",
    "Code explanation:\n",
    "\n",
    "- tf.data.TextLineDataset(data_file): This line read the csv file\n",
    "- .skip(1) : skip the header\n",
    "- .map(parse_csv)): parse the records into the tensors. You need to define a function to instruct the map object. You can call this function parse_csv.\n",
    "\n",
    "This function parses the csv file with the method tf.decode_csv and declares the features and the label. The features can be declared as a dictionary or a tuple. You use the dictionary method because it is more convenient.\n",
    "\n",
    "Code explanation:\n",
    "\n",
    "- tf.decode_csv(value, record_defaults= RECORDS_ALL): the method decode_csv uses the output of the TextLineDataset to read the csv file. record_defaults instructs TensorFlow about the columns type.\n",
    "- dict(zip(_CSV_COLUMNS, columns)): Populate the dictionary with all the columns extracted during this data processing\n",
    "- features.pop('median_house_value'): Exclude the target variable from the feature variable and create a label variable\n",
    "\n",
    "The Dataset needs further elements to iteratively feeds the Tensors. Indeed, you need to add the method repeat to allow the dataset to continue indefinitely to feed the model. If you don't add the method, the model will iterate only one time and then throw an error because no more data are fed in the pipeline.\n",
    "\n",
    "After that, you can control the batch size with the batch method. It means you tell the dataset how many data you want to pass in the pipeline for each iteration. If you set a big batch size, the model will be slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Create the iterator\n",
    "\n",
    "Now you are ready for the second step: create an iterator to return the elements in the dataset.\n",
    "\n",
    "The simplest way of creating an operator is with the method make_one_shot_iterator.\n",
    "\n",
    "After that, you can create the features and labels from the iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Consume the data\n",
    "\n",
    "You can check what happens with input_fn function. You need to call the function in a session to consume the data. You try with a batch size equals to 1.\n",
    "\n",
    "Note that, it prints the features in a dictionary and the label as an array.\n",
    "\n",
    "It will show the first line of the csv file. You can try to run this code many times with different batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-94cb07998201>:16: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "({'crim': array([2.3004], dtype=float32), 'zn': array([0.], dtype=float32), 'indus': array([19.58], dtype=float32), 'nox': array([0.605], dtype=float32), 'rm': array([6.319], dtype=float32), 'age': array([96.1], dtype=float32), 'dis': array([2.1], dtype=float32), 'tax': array([403.], dtype=float32), 'ptratio': array([14.7], dtype=float32)}, array([23.8], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "next_batch = input_fn(df_train, batch_size = 1, num_epoch = None)\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "    print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Define the feature column\n",
    "\n",
    "You need to define the numeric columns as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1= tf.feature_column.numeric_column('crim')\n",
    "X2= tf.feature_column.numeric_column('zn')\n",
    "X3= tf.feature_column.numeric_column('indus')\n",
    "X4= tf.feature_column.numeric_column('nox')\n",
    "X5= tf.feature_column.numeric_column('rm')\n",
    "X6= tf.feature_column.numeric_column('age')\n",
    "X7= tf.feature_column.numeric_column('dis')\n",
    "X8= tf.feature_column.numeric_column('tax')\n",
    "X9= tf.feature_column.numeric_column('ptratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to combined all the variables in a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_columns = [X1, X2, X3,X4, X5, X6,X7, X8, X9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Build the model\n",
    "\n",
    "You can train the model with the estimator LinearRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'train2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E51C664DC8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.LinearRegressor(feature_columns = base_columns, model_dir = 'train2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to use a lambda function to allow to write the argument in the function inpu_fn. If you don't use a lambda function, you cannot train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:518: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\zhang\\Anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train2\\model.ckpt.\n",
      "INFO:tensorflow:loss = 83729.64, step = 1\n",
      "INFO:tensorflow:global_step/sec: 119.532\n",
      "INFO:tensorflow:loss = 13909.657, step = 101 (0.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.055\n",
      "INFO:tensorflow:loss = 12881.449, step = 201 (0.782 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.378\n",
      "INFO:tensorflow:loss = 12391.541, step = 301 (0.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.238\n",
      "INFO:tensorflow:loss = 12050.5625, step = 401 (0.740 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.128\n",
      "INFO:tensorflow:loss = 11766.134, step = 501 (0.768 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.248\n",
      "INFO:tensorflow:loss = 11509.922, step = 601 (0.762 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.727\n",
      "INFO:tensorflow:loss = 11272.889, step = 701 (0.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.934\n",
      "INFO:tensorflow:loss = 11051.9795, step = 801 (0.758 sec)\n",
      "INFO:tensorflow:global_step/sec: 123.028\n",
      "INFO:tensorflow:loss = 10845.855, step = 901 (0.814 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into train2\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5925.9873.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressor at 0x1e51c734dc8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the estimator\n",
    "model.train(steps =1000,    \n",
    "          input_fn= lambda : input_fn(df_train, batch_size=128, num_epoch = None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can evaluate the fit of you model on the test set with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-23T17:21:25Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train2\\model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-23-17:21:26\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 32.15896, global_step = 1000, label/mean = 22.08, loss = 3215.896, prediction/mean = 22.404533\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: train2\\model.ckpt-1000\n",
      " average_loss, was: 32.158958435058594\n",
      " label/mean, was: 22.079999923706055\n",
      " loss, was: 3215.89599609375\n",
      " prediction/mean, was: 22.40453338623047\n",
      " global_step, was: 1000\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(steps = None, \n",
    "                       input_fn = lambda: input_fn(df_eval, batch_size = 128, num_epoch = 1))\n",
    "for key in results:\n",
    "    print(\" {}, was: {}\".format(key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is predicting the value of based on the value of , the matrices of the features. You can write a dictionary with the values you want to predict. Your model has 9 features so you need to provide a value for each. The model will provide a prediction for each of them.\n",
    "\n",
    "In the code below, you wrote the values of each features that is contained in the df_predict csv file.\n",
    "\n",
    "You need to write a new input_fn function because there is no label in the dataset. You can use the API from_tensor from the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = {\n",
    "          'crim': [0.03359,5.09017,0.12650,0.05515,8.15174,0.24522],\n",
    "          'zn': [75.0,0.0,25.0,33.0,0.0,0.0],\n",
    "          'indus': [2.95,18.10,5.13,2.18,18.10,9.90],\n",
    "          'nox': [0.428,0.713,0.453,0.472,0.700,0.544],\n",
    "          'rm': [7.024,6.297,6.762,7.236,5.390,5.782],\n",
    "          'age': [15.8,91.8,43.4,41.1,98.9,71.7],\n",
    "          'dis': [5.4011,2.3682,7.9809,4.0220,1.7281,4.0317],\n",
    "          'tax': [252,666,284,222,666,304],\n",
    "          'ptratio': [18.3,20.2,19.7,18.4,20.2,18.4]\n",
    "     }\n",
    "\n",
    "def test_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensors(prediction_input)    \n",
    "    return dataset\n",
    "     \n",
    "# Predict all our prediction_input\n",
    "pred_results = model.predict(input_fn = test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly, you print the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train2\\model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "(0, {'predictions': array([32.297546], dtype=float32)})\n",
      "(1, {'predictions': array([18.96125], dtype=float32)})\n",
      "(2, {'predictions': array([27.270979], dtype=float32)})\n",
      "(3, {'predictions': array([29.299236], dtype=float32)})\n",
      "(4, {'predictions': array([16.436684], dtype=float32)})\n",
      "(5, {'predictions': array([21.460876], dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "for pre in enumerate(pred_results):\n",
    "    print(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "To train a model, you need to:\n",
    "\n",
    "- Define the features: Independent variables: X\n",
    "- Define the label: Dependent variable: y\n",
    "- Construct a train/test set\n",
    "- Define the initial weight\n",
    "- Define the loss function: MSE\n",
    "- Optimize the model: Gradient descent\n",
    "- Define:\n",
    "  - Learning rate\n",
    "  - Number of epoch\n",
    "  - Batch size\n",
    " \n",
    "In this tutorial, you learned how to use the high level API for a linear regression estimator. You need to define:\n",
    "\n",
    "1. Feature columns. If continuous: tf.feature_column.numeric_column(). You can populate a list with python list comprehension\n",
    "2. The estimator: tf.estimator.LinearRegressor(feature_columns, model_dir)\n",
    "3. A function to import the data, the batch size and epoch: input_fn()\n",
    "\n",
    "After that, you are ready to train, evaluate and make prediction with train(), evaluate() and predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
